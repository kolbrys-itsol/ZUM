{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "394/395 [============================>.] - ETA: 0s - loss: 0.3263 - accuracy: 0.8407\n",
      "Epoch 1: val_accuracy improved from -inf to 0.92740, saving model to best_model.hdf5\n",
      "395/395 [==============================] - 13s 30ms/step - loss: 0.3261 - accuracy: 0.8407 - val_loss: 0.1600 - val_accuracy: 0.9274\n",
      "Epoch 2/10\n",
      "394/395 [============================>.] - ETA: 0s - loss: 0.1634 - accuracy: 0.9308\n",
      "Epoch 2: val_accuracy improved from 0.92740 to 0.93950, saving model to best_model.hdf5\n",
      "395/395 [==============================] - 13s 32ms/step - loss: 0.1632 - accuracy: 0.9309 - val_loss: 0.1380 - val_accuracy: 0.9395\n",
      "Epoch 3/10\n",
      "395/395 [==============================] - ETA: 0s - loss: 0.1358 - accuracy: 0.9440\n",
      "Epoch 3: val_accuracy did not improve from 0.93950\n",
      "395/395 [==============================] - 13s 33ms/step - loss: 0.1358 - accuracy: 0.9440 - val_loss: 0.1530 - val_accuracy: 0.9302\n",
      "Epoch 4/10\n",
      "395/395 [==============================] - ETA: 0s - loss: 0.1200 - accuracy: 0.9521\n",
      "Epoch 4: val_accuracy improved from 0.93950 to 0.94235, saving model to best_model.hdf5\n",
      "395/395 [==============================] - 12s 31ms/step - loss: 0.1200 - accuracy: 0.9521 - val_loss: 0.1408 - val_accuracy: 0.9423\n",
      "Epoch 5/10\n",
      "395/395 [==============================] - ETA: 0s - loss: 0.1073 - accuracy: 0.9577\n",
      "Epoch 5: val_accuracy did not improve from 0.94235\n",
      "395/395 [==============================] - 12s 31ms/step - loss: 0.1073 - accuracy: 0.9577 - val_loss: 0.1623 - val_accuracy: 0.9295\n",
      "Epoch 6/10\n",
      "395/395 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9598\n",
      "Epoch 6: val_accuracy did not improve from 0.94235\n",
      "395/395 [==============================] - 13s 33ms/step - loss: 0.1014 - accuracy: 0.9598 - val_loss: 0.1533 - val_accuracy: 0.9338\n",
      "Epoch 7/10\n",
      "394/395 [============================>.] - ETA: 0s - loss: 0.0918 - accuracy: 0.9645\n",
      "Epoch 7: val_accuracy did not improve from 0.94235\n",
      "395/395 [==============================] - 14s 35ms/step - loss: 0.0917 - accuracy: 0.9646 - val_loss: 0.1622 - val_accuracy: 0.9324\n",
      "Epoch 8/10\n",
      "395/395 [==============================] - ETA: 0s - loss: 0.0870 - accuracy: 0.9657\n",
      "Epoch 8: val_accuracy did not improve from 0.94235\n",
      "395/395 [==============================] - 13s 32ms/step - loss: 0.0870 - accuracy: 0.9657 - val_loss: 0.1626 - val_accuracy: 0.9317\n",
      "Epoch 9/10\n",
      "395/395 [==============================] - ETA: 0s - loss: 0.0824 - accuracy: 0.9662\n",
      "Epoch 9: val_accuracy did not improve from 0.94235\n",
      "395/395 [==============================] - 13s 32ms/step - loss: 0.0824 - accuracy: 0.9662 - val_loss: 0.1669 - val_accuracy: 0.9338\n",
      "Epoch 10/10\n",
      "395/395 [==============================] - ETA: 0s - loss: 0.0797 - accuracy: 0.9699\n",
      "Epoch 10: val_accuracy did not improve from 0.94235\n",
      "395/395 [==============================] - 13s 33ms/step - loss: 0.0797 - accuracy: 0.9699 - val_loss: 0.1619 - val_accuracy: 0.9345\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import pad_sequences, to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data = pd.read_csv('tagged_data.csv')\n",
    "max_words=5000\n",
    "max_len=200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data.Tweets)\n",
    "sequences = tokenizer.texts_to_sequences(data.Tweets)\n",
    "tweets = pad_sequences(sequences,maxlen=max_len)\n",
    "labels = to_categorical(data.cluster,num_classes=2)\n",
    "\n",
    "x_train, x_test, y_train,y_test = train_test_split(tweets,labels,test_size=0.1,stratify=labels, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_words,40))\n",
    "model.add(layers.LSTM(20,dropout=0.5))\n",
    "model.add(layers.Dense(2,activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"best_model.hdf5\",\n",
    "                              monitor='val_accuracy',\n",
    "                              verbose=1,\n",
    "                              save_best_only=True,\n",
    "                              mode='auto',\n",
    "                              save_freq='epoch',\n",
    "                              save_weights_only=False)\n",
    "\n",
    "history = model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test), callbacks=[checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
